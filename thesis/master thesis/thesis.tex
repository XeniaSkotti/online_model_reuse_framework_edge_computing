%% example.tex
%% Jeremy Singer
%% 16 Oct 12

\documentclass{mpaper}
% \usepackage{natbib}


\begin{document}
\title{Model Reuse Framework for Edge Computing}
\author{Xenia Skotti}
\matricnum{2299606s}

\maketitle

\begin{abstract}

According to Simon Peyton Jones, an abstract should address
four key questions. First, what is the problem that this
paper tackles? Second, why is this an interesting problem?
Third, what is the solution this paper proposes?
Finally, why is the proposed solution a good one?
\end{abstract}

% Sharing models in a pool raises concerns including user privacy \cite{ComputeReuse}\cite{Learnware} and intellectual property (IP) considerations \cite{DNNSimilarity}. These are legitimate concerns and one of the ways this can be eliminated is by not sharing the data used to train the models and instead creating a proxy for them \cite{DNNSimilarity}\cite{KernelMMD}.

\section{Introduction}

Lee et al. \cite{ComputeReuse} define compute reuse as "the partial or full utilization of already executed computational task results by multiple users to complete a new task while avoiding computation redundancy".  Systems that adopt compute reuse benefit from significant performance gains motivating model reuse in machine learning (ML). Model reuse \cite{Learnware} attempts to construct a model from other pre-existing and pretrained models for other tasks, in order to avoid building a model from scratch. Exploitation of pre-existing models can set a good basis for the training of a new model which translates into a reduced time cost, data amount and expertise required to train a new model. Moreover, model reuse has been used to tackle concept drift \cite{ConceptDrift} and building ad-hoc analytic models \cite{MaterializationReuse}.

Model reusability is compelling and therefore both theoretical \cite{Learnware} and empirical \cite{MaterializationReuse}\cite{KernelMMD}  frameworks have been proposed to take advantage of it. Many of the approaches proposed, involve a two-phased framework of a preprocessing and runtime phase. In the preprocessing phase, the model and its data are shared in a pool from which in the runtime phase the relevant ML models are identified. Consider the case of edge computing, where given a number of nodes and their corresponding datasets we want to decide for which nodes to train a distinct model and for which to reuse one. In this context the reuse comes from the fact that we don’t train a model for all the nodes but instead reuse one of the existing ones. A framework for model reuse in edge computing requires it is online hence, these steps are merged and to the best of our knowledge no such framework has been proposed. 

One of the fundamental requirements of any model reuse framework is to be able to choose the model that best fits the (test) data of the target domain. One of the ways this can be achieved is by finding the model whose source domain (training data) is drawn from the same distribution as the target domain. Therefore, the difference between domains needs to be quantified and minimised to find the best model. This is essentially what the Maximum Mean Discrepancy (MMD) \cite{OriginalMMD} statistic does. In addition to measuring the similarity between two dataset domains, we need to determine the direction of reusability. In other frameworks where the reused model originated from a pool there was no such requirement because there was only one direction of reusability, the pool. In this setting though there are two directions per pair, and we need to define a method to do so. A simple solution to this, is to measure the overlap between the inlier points of two datasets. Any dataset is expected to have a few outliers and a simple filtering technique would be to use One-class Support Vector Machines (OCSVM) \cite{OriginalOCSVM} to determine which points are inliers. Therefore, given two nodes and their corresponding OCSVM models, we can use each OCSVM model to predict the other node's inliers and then find the probability of detecting them, hence their overlap. 

To summarise, our project contributes a novel online framework for model reuse in edge computing, which given a set of nodes and their corresponding datasets can determine for which nodes to train distinct models and for which nodes to reuse one. 


% This paper outlines the standard template for an MSci submission.
% In earlier years, MSci students at the School of Computing
% Science\footnote{\url{http://www.dcs.gla.ac.uk}},
% University of Glasgow, were expected to produce a full-length
% dissertation. Now, the requirement is for MSci students to
% write a paper of up to 14 pages in length, using the supplied
% \texttt{mpaper} \LaTeX style file.

% The precise structure of an MSci paper is not mandated, but it should
% probably cover in detail the following aspects of the project.
% \begin{enumerate}
% \item General description of the problem, motivation, relevance
% \item Background information, possibly including a literature survey
% \item Description of approach taken to solve the problem, including
%   high-level design and lower-level implementation details as appropriate
% \item Evaluation, qualitative or quantitative as appropriate
% \item Conclusion, including scope for future work
% \end{enumerate}

\section{Background}

\subsection{Maximum Mean Discrepancy}\label{chap2:MMD}

Maximum mean discrepancy (MMD) is a statistic that can quantify the mean discrepancy of two data distributions in a kernel space in order to determine if two samples are drawn from different distributions \cite{OriginalMMD}. Let $p$ and $q$ be two independent probability distributions, and $E_x\left[f\left(x\right)\right]$ (shorthand notation for $E_{x~p}\left[f\left(x\right)\right]$) denotes the mathematical expectation of $f\left(x\right)$ with $x$ under the probability density $p$. The statistic definition between $p$ and $q$ is:

\begin{equation}
\begin{aligned}
    	MMD\left( \boldsymbol{\mathcal{F}},p, q \right) & = \sup_{f\ \in\ \boldsymbol{\mathcal{F}}} {\left( E_x \left[ f\left( x \right) \right] -E_y \left[ f \left( y \right) \right] \right)} \\
    	& = {\sup_{f\ \in\ \boldsymbol{\mathcal{F}}}{\langle f,\mu_p-\mu_q\rangle_{\boldsymbol{\mathcal{H}}}}}
\end{aligned}
\end{equation}
where the function class $\boldsymbol{\mathcal{F}}$ is a unit ball in the reproducing Hilbert space (RKHS) and $\mu_p$, $\mu_q$ is the mean embedding of $p$ and $q$ respectively i.e., the mean of the feature mapping in the kernel space. The function class $\boldsymbol{\mathcal{F}}$ is universal meaning that $MMD\left(\boldsymbol{\mathcal{F}},p,q\right)=0$ if and only if $p=q$. Therefore, MMD is the largest difference in expectations over functions in $\boldsymbol{\mathcal{F}}$ and can only be zero if the two samples were drawn from the same distribution.

In practise, we use the square MMD in order to be able to use kernel functions. Let $X=\left\{x_1,...,x_m\right\}$ and $Y=\left\{y_1,...,y_n\right\}$ denote the independent and identically distributed (i.i.d.) samples from distribution $p$ and $q$ respectively. An unbiased estimation of $MMD^2 \left( \parallel{\mu_p-\mu_q}\parallel^2_{\boldsymbol{\mathcal{H}}} \right)$ can be obtained using a U-statistic:

\begin{equation}
\begin{aligned}
	MMD^2\left(\mathbf{F},p,q\right) = &\frac{1}{m(m-1)}\sum_{i=1}^{m}\sum_{j\neq i}^{m}k\left(x_i,x_j\right) + \\
	& \frac{1}{n(n-1)}\sum_{i=1}^{n}\sum_{j\neq i}^{n}k\left(y_i,y_j\right) - \\
	& \frac{2}{mn}\sum_{i=1}^{m}\sum_{j=1}^{n}k\left(x_i,y_j\right) 
\end{aligned}
\end{equation}
where $k(.)$ denotes the kernel function. In our experiments we've utilised the linear and Gaussian RBF kernel, a special case of when data are centered around the origin, as defined below:

\begin{equation}\label{eqn:linear}
    k(x,y) = x^Ty\ (Linear)
\end{equation}
\begin{equation}\label{eqn:rbf}
    k(x,y) = exp\left(-\frac{1}{2\sigma^2}\parallel x - y \parallel^2\right)\ (RBF)
\end{equation}
where $\sigma \in \mathbb{R}$ is a kernel parameter and $\parallel x - y\parallel$ is a dissimilarity measure such as the square Euclidean distance. 

\subsection{One-class Support Vector Machines}

One-class support vector machines (OCSVMs) is a one-class classification technique, which aims to classify instances into one of two classes, the inlier and outlier class. The method, first presented by Schölkopf et. al \cite{OriginalOCSVM},  utilizes a training data set with normal data to learn the boundaries of the normal data points. Therefore, data points which lie outside of the region to be classified as outliers. OCSVMs utilize an implicit transformation function $\phi\left(.\right)$ defined by the kernel to project data to a higher dimensional space. The algorithm learns the decision boundary (a hyperplane) which achieves the maximum separation of the majority of the data points from their origin. Only a small fraction of data points are allowed to lie on the other side of the decision boundary and those data are considered outliers. 

The OCSVM algorithm returns a function $f$ that takes the value +1 for the normal region and -1 elsewhere. Hence, function $f$ is called a decision function and is defined as:

\begin{equation}
    f(x) = sign(g(x)) = sign(w^T\phi(x) - \rho)
\end{equation}
where $w$ is the vector perpendicular to the decision boundary ($g(x) = 0 $) and $\rho$ is the bias term. 
Given that the distance of any arbitrary data point to the decision boundary can be calculated with the following:

\begin{equation}
    d(x) = \frac{|g(x)|}{\parallel w \parallel}
\end{equation}
and the fact that the origin's value when plugged to $g(x)$ is $\rho$, the distance of the origin to the decision boundary is $\frac{\rho}{\parallel w\parallel}$. The OCSVM algorithm essentially attempts to maximise the distance by solving the minimisation problem of $\frac{\parallel w \parallel}{2} - \rho$.

Formally the primary objective of OCSVM is defined by the following equation:

\begin{equation}\label{eqn:7}
    \min_{w, \xi \in \mathbb{R}^N, \rho\in\mathbb{R}} \frac{\parallel{w}\parallel^2}{2} - \rho + \frac{1}{vn}\sum_{i=1}^n \xi_i 
\end{equation}
\begin{center}
subject to $(w^T \cdot \Phi(x_i)) \geq \rho - \xi_i, \xi \geq 0$
\end{center}
where $\xi_i$ is the slack variable for a point i which allows it to lie on the other side of the decision boundary, n is the size of the training dataset and $v \in (0,1)$ is the regularization parameter. As shown in Equation \ref{eqn:7} the objective is not only to minimise the distance of the origin to the decision boundary but also minimise the slack variables $\xi_i$ for all points. The regularization parameter $v$ represents the upper bound limit of the fraction of outliers and a lower bound on the number of support vectors. In other words, $v$ specifies the number of training points which are guaranteed to be misclassified and the number of training examples being support vectors. As mentioned above $v \in (0,1)$ and therefore a percentage, where a high value may lead to over-fitting and a low value to under-fitting. The $v$ value controls the trade off between $\xi$ and $\rho$.

In order to reduce the number of variables to a single vector and utilise the kernel trick, the primary objective is transformed into a dual objective:
\begin{equation}
    \min_a \frac{a^TQa}{2}
\end{equation}
\begin{center}
    subject to: $0 \leq a_i \leq \frac{1}{vn}, \sum_{i=1}^n a_i = 1$
\end{center}
where $Q$ is the kernel matrix and $a$ the Lagrange multipliers. Using the above, the decision function now becomes:

\begin{equation}
    f(x) = sign(\sum_{i=1}^n a_i k(x, x_i))
\end{equation}
For OCSVM we've utilised the RBF kernel as defined by Equation \ref{eqn:rbf}.

% \subsection{Support Vector Regression Machines}

% A version of SVM for regression was proposed by Vapnik et al. \cite{OriginalSVR} called Support Vector Regression (SVR). The adaptation is accomplished by introducing an $\epsilon$-insensitive region around the function, called the $\epsilon$-tube as shown in Figure \ref{fig-SVRex}. Similarly to other SVM methods, the hyperplane is represented in terms of support vectors, which are training samples that lie outside the boundary of the tube. Support vectors are the most influential instances that affect the shape of the tube, and the training and test data are assumed to be i.i.d., drawn from the same fixed but unknown probability distribution function in a supervised-learning context.

% The optimization problem objective is to find the tube that best approximates the continuous-valued function, while balancing model complexity and prediction error. Therefore, the goal is to first minimise an $\epsilon$-insensitive loss function and find the flattest tube that contains most of the training instances. The $\epsilon$-insensitive loss function $L_1$ ($L_1=L(y_i, f(x_i,w))$) is defined as:
% \begin{equation}\label{eqn:10}
%     L_1 = 
%     \begin{cases}
%         0, & if\ |y_i- f(x_i, w)| - \epsilon \\
%         |y_i- f(x_i,w)|- \epsilon, & otherwise
%     \end{cases}
% \end{equation}
% where $y_i$ represents the true value and $f(x_i,w)$ the predicted value for a given input $x_i$ respectively. if $f(x_i,w)$ is within the tube the loss is zero, otherwise the loss is the magnitude of the difference between the predicted value and the radius $\epsilon$ of the tube.

% \begin{figure}
% \begin{center}
% \includegraphics[scale=0.45]{SVR_representation.jpg}
% \end{center}
% \caption{\label{fig-SVRex}Visual Representation of an SVR}
% \end{figure}

% A multi-objective function is constructed from the loss function and the geometrical properties of the tube:

% \begin{equation}\label{eqn:11}
%     \min_{w, \xi, \hat{\xi} \in \mathbb{R}}{\frac{\parallel{w}\parallel^2}{2} + C\sum_{i=1}^n (\xi_i + \hat{\xi_i})}
% \end{equation}
% \begin{center}
% subject to $\xi, \hat{\xi} \geq 0$, i=1,...,n \\ 
% $w^T \cdot \Phi(x_i) + b - y_i \leq \epsilon + \xi_i$\\ 
% $y_i - w^T \cdot \Phi(x_i) - b \leq  \epsilon + \hat{\xi_i} $ \\
% \end{center}
% where $\Phi(.)$ is the transformation to a kernel space and C is a regularization parameter which controls the strength of the penalty. C acts as an inverse regularization parameter since when it is large, more emphasis is placed on the error and when the opposite is true, more emphasis is placed on the norm of the weights. The values of $\xi$ and $\hat{\xi}$ follow a similar pattern as Equation \ref{eqn:10} where their values are zero if $y_i$ is inside the tube, otherwise $\xi$ is the positive difference between $y_i$ and $\epsilon$ and $\hat{\xi}$ will be nonzero if $y_i$ is below the tube. 

% The corresponding Langrarian ($L_2=L(w,\xi,\hat{\xi},\lambda,\hat{\lambda},\alpha,\hat{\alpha})$) of Equation \ref{eqn:11}:

% \begin{equation}\label{eqn:12}
% \begin{aligned}
%         L_2 = &  {\frac{\parallel{w}\parallel^2}{2} + C\sum_{i=1}^n (\xi_i + \hat{\xi_i})} - \\
%         & {\sum_{i=1}^N\hat{\alpha_i}(y_i- w^T \cdot \Phi(x_i) -b + \epsilon + \hat{\xi_i})} - \\
%         & {\sum_{i=1}^N\alpha_i(w^T \cdot \Phi(x_i) + b - y_i + \epsilon + \xi_i)} - \\
%         & \sum_{i=1}^N(\lambda_i\xi_i+ \hat{\lambda_i}\hat{\xi_i})
% \end{aligned}
% \end{equation}
% where $\lambda,\hat{\lambda},\alpha,\hat{\alpha}$ are the Lagrange multipliers and are all non-negative real numbers. The minimum of Equation \ref{eqn:12} can be found by differentiating with respect to the parameters ($w, b, \xi$) which results in the equivalent maximization of the following dual objective function:

% \begin{equation}
% \begin{aligned}
%     \max_{\alpha,\hat{\alpha}}{y^T (\hat{\alpha} -\alpha) - \epsilon e^T(\hat{\alpha} +\alpha)  - \frac{1}{2}{(\alpha+\hat{\alpha})^TQ(\alpha-\hat{\alpha})}}
% \end{aligned}
% \end{equation}
% \begin{center}
%     subject to $e^T(\alpha-\hat{\alpha})=0$, $0 \leq \alpha_i,\hat{\alpha_i} \leq C, i=1,...,n$
% \end{center}
% where $e$ is a vector of all ones and $Q$ is the kernel matrix ($Q_{ij}=k(x_i,x_j) = (\Phi(x_i)^T \cdot \Phi(x_i))$. Assuming we have a new input $x_p$ we can use the following function to get its prediction value $y_p$:

% \begin{equation}
%     y_p = \sum_{i=1}^{N_{SV}}(\alpha_i-\hat{\alpha})k(x_i,x_p) + b
% \end{equation}
% where $N_{SV}$ denotes the number of support vectors and $k(x_i,$ $x_p)$ the kernel. In our experiments, we've set the SVR kernel to either be a linear or an RBF kernel (Equations \ref{eqn:linear} and \ref{eqn:rbf} respectively).

\section{Related Work}

Compute reuse has been investigated in the context of edge computing by \cite{ComputeReuse} to quantify its gain. Executing experiments on three applications: matrix multiplication, face detection, and chess, they found that systems that adopt compute reuse, compared to systems that don't, can finish the same task up to five times faster. In addition to the benefits of compute reuse they also highlight some challenges including task representation and privacy considerations. Model tasks need to have a clear specification detailing their purpose and speciality in order to identify when they can be re-used while also preserving user privacy when they are shared. Motivated by similar concerns a theoretical paradigm named learnware was proposed by Zhou \cite{Learnware}. More specifically, a learnware is a machine learning model that is pretrained and achieves good performance paired with a detailed specification. The vision behind the paradigm was that learnware models can be shared in a pool without their raw data, allowing data scientists to identify pretrained models that satisfy their requirements without concerns over privacy violations. Therefore, the author identified three characteristics: reusable, evolvable and comprehensible as fundamental for a model to be considered a learnware.  

Based on this paradigm, the reduced kernel mean embedding (RKME) \cite{KernelMMD} was presented, a two phased framework consisting of the upload and deployment phase. During the upload phase, each model is paired with its kernel mean embedding (KME) of the dataset and added to the pool of models. Roughly speaking, a kernel mean embedding is a point in the reproducing Hilbert space (RKHS) which "summarises" the probability distribution. Then in the deployment phase either a single or a combination of models is chosen based on the RKHS distance between the testing (target) mean embedding and reduced (source) embedding of pool models. Therefore, there is no need to access the raw data since KME acts a proxy for them. The RKME method is similar to the MMD  statistic \cite{OriginalMMD}, which is the largest difference between the mean embedding of two populations (source and target) and its aim is to determine if the two populations were drawn from the same distribution. Essentially, this is what the deployment phase of the framework does, it wants to find the model which minimises the difference and thus ensures that the target distribution is the same as the source. The framework was tested in a series of experiments including a real-world project where it outperformed reuse baselines in terms of the root-mean-square error.

The author of the learnware paradigm \cite{Learnware} recognises transfer learning as a preliminary attempt to reusability. The aim of transfer learning is to transfer the knowledge of a pretrained model to a new model that is used for a different but related problem. In transfer learning there are three key research issues as identified in \cite{DefinitionTL}: when, how and what to transfer. This corresponds to identifying a source domain that would benefit the target domain, then using an algorithm the transferable knowledge across domains is discovered. A two-stage framework dubbed as Learning to Transfer (L2T) was presented \cite{L2T}, which exploits previous transfer learning experiences to optimize what and how to transfer between domains. In the first stage each transfer learning experience is encoded into three parts: a pair of source and target domains, the transferred knowledge between them represented by latent factors and the performance improvement ratio. Using these transfer learning experiences, L2T learns a reflection function, which approximates the performance improvement ratio and thus encrypts transfer learning skills of deciding what and how to transfer. The improvement ratio in this framework is the difference between domains calculated by MMD further highlighting the similarity to RKME \cite{KernelMMD}. In addition to the MMD between domains, the variance is also calculated since a small MMD paired with an extremely high variance still indicates little overlap. A potential drawback of the RKME \cite{KernelMMD} framework, and by extension the learnware paradigm, is that the variance between pairs cannot be calculated since the raw data are not available during the testing phase. During the second stage, whenever a new pair of domains arrives, L2T optimizes the knowledge to be transferred by maximising the value of the learned reflection function.

Concerns over intellectual property (IP) infringement and vulnerability propagation of deep learning models (DNN) motivated the proposal of ModelDiff \cite{DNNSimilarity}, a testing-based approach to DNN model similarity comparison. They compare the decision logic of models on the test inputs represented by a decision distance vector (DDV),a newly defined data structure in which each value is the distance between the outputs of the model produced by two inputs. These inputs are pairs of normal and corresponding adversarial samples and thus when used to calculate the DDV, the decision boundary is captured. In contrast to RKME \cite{KernelMMD} which is a compute reuse framework, ModelDiff is a model reuse detector. 

Model reuse has also been used to handle concept drift, a situation where the distribution of the data (usually stream data) changes. The assumption that previous data contain some useful information, indicates that the models corresponding to the data can be leveraged. Condor was proposed \cite{ConceptDrift} as an approach to handling concept drift through model reuse. Condor consists of two modules, ModelUpdate and WeightUpdate which leverage previous knowledge to build new model, hence updating the model pool and adapt the weights of previous models to reflect current reusability performance respectively. The effectiveness of the approach was validated using both synthetic and real-world datasets. 

Hasani et al. \cite{MaterializationReuse} proposed a two-phased approach, to build faster models for a popular class of analytic queries by leveraging model reuse. Similar to other approaches such as RKME \cite{KernelMMD}, there is a preprocessing and a runtime phase. During the first phase the models, their statistics and some meta-data are stored, while in the second phase relevant models are identified from which an approximate model is constructed. Moreover, they propose two methods for generating approximate models, one which is extremely fast but does not provide a fine-tuning option and another which does at the cost of efficiency. Their approach can achieve speed-ups of several orders on magnitude on very large datasets, however it is only geared towards exploratory analysis purposes and the approach is potentially less robust under concept drift. 

Lee et al. \cite{ComputeReuse} also discuss alternative approaches and corresponding challenges of compute reuse including in networks. They identify that reuse can be achieved either in a distributed or centralized manner. The distributed approach involves forwarding tasks to the compute reuse node that is responsible for the operation. This adds additional complexity to the forwarding operations of routers resulting in a potential downgrade in performance. Reuse of results in a network setting undoubtedly improves performance, however speeding up the estimation of parameters can also be beneficial in that regard. Nodes in a network can collaborate to estimate parameters as discussed in \cite{DistributedEstimation}. More specifically, their method takes advantage of the joint sparsity of vectors used for computations enhancing estimation performance. Joint sparsity simply means that the indexes of nonzero entries for all nodes are the same, but their values differ.  The authors also adopt an intertask cooperation strategy to consider intertask similarities. Their method assumes that both the vectors of interest and their associated noise follow a zero-mean Gaussian distribution which is a strong assumption for the data to hold. 

In conclusion, reusing models results in significant reduction in compute usage resources. Both theoretical and empirical frameworks have been proposed to take advantage of the performance improvement of model reusability. Nevertheless, model reuse has also been used to tackle concept drift and building ad-hoc analytic models. While model reuse is undoubtedly beneficial many have raised concerns including user privacy and intellectual property considerations.  These are legitimate concerns of model sharing, however  our model reuse  framework is novel and therefore user privacy is not a concern at this stage of development. In the future we could amend the framework to not expose any data outside of the node. At this stage we've investigated whether the framework is feasible. In contrast to previous research in which frameworks required two distinct steps, our framework is online, and they are therefore merged. Our framework includes determining which datasets are similar, but also the direction of reusability.  Similarly, to the L2T \cite{L2T} framework we use MMD to measure the similarity of two dataset domains. In previous research there was no requirement to determine the direction of reusability hence we propose a novel approach, using the OCSVM model of each node to predict other node’s inliers and measuring the overlap.

% \begin{table}
% \begin{tabular}{l||c||p{2cm}}
% % \emph{Operating System} & \emph{Version} & \emph{Verdict} \\ \hline \hline
% % Ubuntu & 12.04 & Everyone's favourite Linux, unless you grew up with
% % RedHat \\ \hline
% % Slackware & xxx & Pseudo-hacker's Linux, how often do you recompile
% % your kernel? \\ \hline
% % Mac OS & 10.7 & For people with more money than sense \\ \hline
% \end{tabular}
% \caption{\label{tab-eg}Single column table of figures}
% \end{table}

% \begin{lstlisting}[language=python,caption={The algorithm for OCSVM model 1}, label=lst:ocsvm1]
% def avg_similarity_disimilarity_MMD(samples, similar_nodes, other_nodes, 
%                                     kernel, kernel_bandwidth, return_tables = True):
    
%     ## Calculating the baseline ASMMD
%     combos = comb(range(len(similar_nodes)),2)
%     similar_mmds = []
%     for combo in combos:
%         x = similar_nodes[combo[0]]
%         y = similar_nodes[combo[1]]
%         sx = samples[x]
%         sy = samples[y]
%         mmd = MMD(sx,sy, kernel, kernel_bandwidth)
%         similar_mmds.append(mmd)
%         s.add_row([(x,y), mmd])

% \end{lstlisting}

% NOTE: line ends are denoted by \; in algorithm2e

\SetInd{0.1em}{0.5em}
\SetNlSkip{0.3em}

\begin{algorithm}
    \DontPrintSemicolon
    \caption{Calculates the average similarity MMD (ASMMD) between the given nodes.
    }\label{alg:asmmd}
    
    \KwData{$\boldsymbol{kernel}, \boldsymbol{bandwidth}$: the kernel type and scalar value to be used for the MMD calculation, $\boldsymbol{samples}$: dictionary associating each node with a sample, $\boldsymbol{similar\_nodes}$: nodes identified as similar to each other, $\boldsymbol{other\_nodes}$: the rest of the nodes.}
    \KwResult{$\boldsymbol{ASMMD}$}

    \Begin{ 
        \tcp{Calculating the baseline ASMMD}
        $similar\_mmds \longleftarrow \left[\right]$ \;
        \SetAlgoLined
        \SetKw{KwTo}{in}
        \For{$x, y$ \KwTo $get\_pair\_combos(similar\_nodes)$}{
            $sx \longleftarrow samples[x]$, $sy \longleftarrow samples[y]$ \;
            $mmd \longleftarrow MMD(sx, sy, kernel, bandwidth)$ \;
            $similar\_mmds.append(mmd)$ \;
        }
        \tcp{Compare which of the the other\_nodes are similar to the similar\_nodes using the current ASMMD in each iteration}
        \For{$x$ \KwTo $other\_nodes$}{
            $sx \longleftarrow samples[x]$ \;
            \For{$y$ \KwTo $similar\_nodes$}{
                $sy \longleftarrow samples[y]$ \;
                $mmd \longleftarrow MMD(sx, sy, kernel, bandwidth)$ \;
                $asmmd \longleftarrow \boldsymbol{mean}(similar\_mmds)$ \;
                \If{$mmd < (asmmd + 1) * 0.05$}{
                    $similar\_mmds.append(mmd)$}
            }
        }
        \tcp{Which the other\_nodes are similar to each other}
        \If{$\boldsymbol{len}(other\_nodes>1)$}{
            \For{$x, y$ \KwTo $get\_pair\_combos(other\_nodes)$}{
                $sx \longleftarrow samples[x]$, $sy \longleftarrow samples[y]$ \;
                $mmd \longleftarrow MMD(sx, sy, kernel, bandwidth)$ \;
                $asmmd \longleftarrow \boldsymbol{mean}(similar\_mmds)$ \;
                \If{$mmd < (asmmd + 1) * 0.05$}{
                    $similar\_mmds.append(mmd)$}
            }
        }
        $asmmd = \boldsymbol{mean}(similar\_mmds)$ \;
    }
\end{algorithm}

\section{The Framework}\label{chap:framework}

In this section we elaborate and provide a detailed implementation of the framework. Our online model reuse framework needs to be able to determine two things given a pair of nodes. First and foremost, the pairs of nodes which have similar datasets and then the direction of reusability. 



The first objective is achieved using MMD, which measures the difference domains and hence theoretically when the MMD value is zero this means the two datasets are drawn from the same distribution. However, as discussed in section \ref{chap2:MMD} in practise we utilise an estimation of MMD squared. As a consequence, the value is not actually zero and we need to define a threshold below which a pair would be considered similar. We've dubbed the threshold to be the average similarity MMD (ASMMD), a value calculated using Algorithm \ref{alg:asmmd}. 

Algorithm \ref{alg:asmmd} requires that we categorise nodes into two sets, one where all nodes are similar to each other and the rest of them. Categorising nodes in these categories differs when using a regression and classification dataset. We discuss this further in section \ref{chap:asmmd_parameters}. Once we've identified these two sets, we calculate a baseline ASMMD by calculating the MMD of all pair combinations of the similar nodes. Then, we use ASMMD (allowing for a 5\% variation) to judge whether the rest of the nodes are similar to each other or to the similar nodes. If they are we calculate the new ASMMD and we use this to judge the next pair. Using the result of this process we can then judge which pairs are similar for a given experiment as demonstrated in Algorithm  \ref{alg:similar_pairs}. It is worth highlighting that for the MMD implementation to work and by extension all of the algorithms that utilize it (Algorithms \ref{alg:asmmd} \& \ref{alg:similar_pairs}), the samples of each node need to be of equal size.

\begin{algorithm}
    \DontPrintSemicolon
    \caption{Finds the similar pairs of the dataset using MMD
    }\label{alg:similar_pairs}
    
    \KwData{$\boldsymbol{samples}$: dictionary associating each node with a sample, $\boldsymbol{asmmd}$: average similarity (ASMMD) calculated using Algorithm \ref{alg:asmmd}, $\boldsymbol{kernel}, \boldsymbol{bandwidth}$: the kernel type and scalar value to be used for the MMD calculation.}
    \KwResult{$similar\_pairs, pair\_mmds$}
    
    \Begin{
        $similar\_pairs \longleftarrow []$ \;
        % $similar\_nodes \longleftarrow []$ \;
        $pair\_mmds \longleftarrow []$\;
        $nodes \longleftarrow samples.keys()$ \; 
        \SetAlgoLined
        \SetKw{KwTo}{in}
        \For{x, y \KwTo $get\_pair\_combos(nodes)$}{
            $sx \longleftarrow samples[x]$, $sy \longleftarrow samples[y]$ \;
            $mmd \longleftarrow MMD(sx, sy, kernel, bandwidth)$ \;
            \If {$mmd < (asmmd + 1) * 0.05$}{
                % \If{x $\boldsymbol{not\ in}$ similar\_nodes}{
                %     $similar\_nodes.append(x)$
                % }
                % \If{y $\boldsymbol{not\ in}$ similar\_nodes}{
                %     $similar\_nodes.append(y)$
                % }
                $similar\_pairs.append((x,y))$ \;
                $pair\_mmds.append(mmd )$ \;
            }
        }
    }
\end{algorithm}

Once we identify the similar pairs in the network we can then calculate the OCSVM scores of each node in each pair and hence determine the direction of reusability per pair. The OCSVM score is essentially the probability of detecting the inliers of the node by using the other node's model.  Therefore, given two nodes $x$ and $y$, and their corresponding OCSVM models, we use each OCSVM model to predict the other node's inliers and then we calculate the number of points that were identified as inliers and divide by the number points in the dataset, hence the probability. The reason we divide by the number of points in the dataset is because we expect to do some form of filtering prior and remove the outliers if they exist, hence all the points in the dataset are inliers. We calculate the OCSVM score for both directions and whichever is higher is the node for which we should train the model for. Algorithm \ref{alg:ocsvm_scores} calculates of the OCSVM scores of each node per pair.

\begin{algorithm}
    \DontPrintSemicolon
    \caption{Calculates the OCSVM score of each node per pair
    }\label{alg:ocsvm_scores}
    
    \KwData{$\boldsymbol{samples}$: dictionary associating each node with a sample, $\boldsymbol{models}$: dictionary associating each node with its OCSVM node model, $\boldsymbol{similar\_pairs}$: the MMD identified  similar pairs}
    \KwResult{$pair\_prob$}
    
    \Begin{
        $pair\_prob \longleftarrow []$\;
        \SetKw{KwTo}{in}
        \SetAlgoLined
        \For{x, y \KwTo $similar\_pairs$}{
            $sx \longleftarrow samples[x]$, $sy \longleftarrow samples[y]$ \;
            
            $pred\_y\_inliers \longleftarrow get\_inliers(models[x], sy)$, 
            $pred\_x\_inliers \longleftarrow get\_inliers(models[y], sx)$  \;
            
            $x\_y\_overlap \longleftarrow \boldsymbol{len}(pred\_y\_inliers)/\boldsymbol{len}(sy)$,
            $y\_x\_overlap \longleftarrow \boldsymbol{len}(pred\_x\_inliers)/\boldsymbol{len}(sx)$ \;
   
            $pair\_prob.append((x\_y\_overlap,y\_x\_overlap))$ \;
            
        }
    }
\end{algorithm}

\section{Experimental Setup}

\subsection{Training Data}\label{chap:training_data}

We've evaluated our framework for both regression and classification. 

% \begin{figure}
% \centering
%     \begin{subfigure}
%       \includegraphics[width=1\linewidth]{experiment_1_pairplot.png}
%     \end{subfigure}
%     \begin{subfigure}
%       \includegraphics[width=1\linewidth]{experiment_2_pairplot.png}
%     \end{subfigure}   
%     \begin{subfigure}
%       \includegraphics[width=1\linewidth]{experiment_3_pairplot.png}
%     \end{subfigure}
%     \caption{\label{fig:gnfuv_pairplots} The relationship between humidity and temperature per experiment alongside their distribution plots}
% \end{figure}

For regression, we've used the \textbf{\textit{GNFUV Unmanned Surface Vehicles Sensor Data Set}} \cite{GNFUV} which includes data from three experiments. In each experiment there are four sets of mobile sensor readings data recorded by the Raspberry Pi's corresponding to four Unmanned Surface Vehicles (USVs). Each node (USV) dataset contains the humidity and temperature recorded when they were floating over the sea surface in a GPS pre-defined trajectory in a coastal area of Athens (Greece). The data description can be found in Table \ref{tab:gnfuv}. 

For classification we've used the \textbf{\textit{UCI Bank Marketing Dataset}} (BM) \cite{BMDataset} which consists of four files of which we've used the bank-additional-full.csv one which contains 41188 records and 20 attributes. The data was collected by a banking institution through phone calls as part of a direct marketing campaign. The dataset is a binary classification dataset of classes "yes" or "no", to subscribe to the product (bank term deposit). To get a yes from a client often more than one contact was required hence class imbalance is present. More specifically there are 4640 yes instances and 36548 no instances. 

As mentioned above the dataset has 20 attributes hence we've applied Principal Component Analysis (PCA) to reduce the number of dimensions. We've transformed data through PCA and kept 3 of the 20 dimensions and then subsequently used these data to execute the hypothesis testing. In comparison to the GNFUV dataset the BM dataset has no inherent network-node like structure and hence it was constructed by us. We trained a K-means classifier with an equal number of yes and no instances (9280 total) to split data into four clusters. This was done to avoid class imbalance from influencing the clustering algorithm. However, we wanted to have more available samples to split into more nodes so instead of clustering equal amounts of instances per class, we used three times the number of yes instances for no instances (4640 (yes) 13920 (no)). We left cluster 3 shown in Figure \ref{fig-BMcl}, which was large enough and has a good class balance, untouched and merged the other three clusters together to create a larger cluster. From these clusters we then created 5 nodes in total and the node data description can be found in Table \ref{tab:BMnodes}. 

It is worth mentioning we've used two data configurations per dataset. For the GNFUV dataset the two configurations were the original data and a standardised version of them. While for the BM Dataset we used the node data created from the process above as well as a balanced version of them, by under sampling the majority class (no) to have an equal number of instances as the minority class (yes). In each case this was done to explore the effect of the data configuration change on the effectiveness of the framework. 

Lastly, we've drawn 100 unique samples per network, in each of which the node data have an equal number of examples in order to comply with the MMD implementation constraint discussed in section \ref{chap:framework}. The sample size of each node dataset is determined by the minimum sample size (MSS) of the network i.e. the node with the minimum number of entries. If the two thirds of the MSS is more than 500 instances the sample size is the two-thirds of the MSS, otherwise it is 500, unless the MSS is less than that in which case the sample size is equal to the it. Even though for some nodes we're not really creating a sample but simply taking all or most the points of the node, the remainder of the nodes will be samples and therefore we're testing a different relationship between nodes with each sample and hence we do not compromise the validity of our experiment.

\begin{figure}
\centering
\begin{center}
\includegraphics[scale=0.65]{gnfuv_pairplots.JPG}
\end{center}
\caption{\label{fig:gnfuv_pairplots} The relationship between humidity and temperature per experiment alongside their distribution plots for the original GNFUV data}
\end{figure}

\begin{figure}
\centering
\begin{center}
\includegraphics[scale=0.65]{gnfuv_pairplots_std.JPG}
\end{center}
\caption{\label{fig:gnfuv_pairplots} The relationship between humidity and temperature per experiment alongside their distribution plots for the standardised GNFUV data}
\end{figure}

\begin{figure}
\centering
\begin{center}
\includegraphics[scale=0.7]{BM Clustering.JPG}
\end{center}
\caption{\label{fig-BMcl}The Result of clustering on the BM Dataset}
\end{figure}

\begin{table*}[]
\centering
\begin{tabular}{|ccc|cccc|}
\hline
\multicolumn{1}{|c|}{\textbf{Dataset}}                & \multicolumn{1}{c|}{\textbf{Experiment}} & \textbf{Data Configuration} & \multicolumn{4}{c|}{\textbf{ASMMD Algorithm Parameters}}                                                                                              \\ \hline
\multicolumn{1}{|l}{}                                 & \textbf{}                                & \textbf{}          & \multicolumn{1}{c|}{\textbf{similar\_nodes}} & \multicolumn{1}{c|}{\textbf{other\_nodes}} & \multicolumn{1}{c|}{\textbf{kernel}} & \textbf{bandwidth} \\ \hline
\multicolumn{1}{|c|}{\multirow{6}{*}{\textbf{GNFUV}}} & \multicolumn{1}{c|}{\multirow{2}{*}{1}}  & standardised       & \multicolumn{1}{c|}{pi2, pi3, pi4}           & \multicolumn{1}{c|}{pi5}                   & \multicolumn{1}{c|}{rbf}             & 0.5                \\
\multicolumn{1}{|c|}{}                                & \multicolumn{1}{c|}{}                    & original           & \multicolumn{1}{c|}{pi2, pi4}                & \multicolumn{1}{c|}{pi3, pi5}              & \multicolumn{1}{c|}{rbf}             & 10                 \\ \cline{2-7} 
\multicolumn{1}{|c|}{}                                & \multicolumn{1}{c|}{\multirow{2}{*}{2}}  & standardised       & \multicolumn{1}{c|}{pi2, pi3, pi5}           & \multicolumn{1}{c|}{pi4}                   & \multicolumn{1}{c|}{rbf}             & 1                  \\
\multicolumn{1}{|c|}{}                                & \multicolumn{1}{c|}{}                    & original           & \multicolumn{1}{c|}{pi3, pi5}                & \multicolumn{1}{c|}{pi2, pi4}              & \multicolumn{1}{c|}{rbf}             & 100                \\ \cline{2-7} 
\multicolumn{1}{|c|}{}                                & \multicolumn{1}{c|}{\multirow{2}{*}{3}}  & standardised       & \multicolumn{1}{c|}{pi2, pi4}                & \multicolumn{1}{c|}{pi3, pi5}              & \multicolumn{1}{c|}{rbf}             & 1                  \\
\multicolumn{1}{|c|}{}                                & \multicolumn{1}{c|}{}                    & original           & \multicolumn{1}{c|}{pi2, pi4}                & \multicolumn{1}{c|}{pi3, pi5}              & \multicolumn{1}{c|}{rbf}             & 5                  \\ \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{BM}}}    & \multicolumn{1}{c|}{}                    & balanced           & \multicolumn{1}{c|}{pi1, pi2, pi3}           & \multicolumn{1}{c|}{pi4, pi5}              & \multicolumn{1}{c|}{linear}          & 0.001              \\
\multicolumn{1}{|c|}{}                                & \multicolumn{1}{c|}{}                    & unbalanced         & \multicolumn{1}{c|}{pi4, pi5}                & \multicolumn{1}{c|}{pi1, pi2, pi3}         & \multicolumn{1}{c|}{linear}          & 0.001              \\ \hline
\end{tabular}
\caption{\label{tab:asmmd_parameters} ASMMD Algorithm Parameters per Dataset}
\end{table*}

\subsection{ASMMD Algorithm Parameters}\label{chap:asmmd_parameters}

As discussed in Section \ref{chap:framework}, the ASMMD Algorithm  takes four arguments, the sample of each node, the kernel, bandwidth, the similar and other nodes. In this section we discuss how we set and what the kernel, bandwidth and similar and other nodes are per dataset (and experiment in the case of the GNFUV dataset). 

The approach to identifying the similar and other nodes for each dataset differed due to the nature of each dataset. Since the GNFUV is a regression dataset of only two dimensions, we plotted the points of each experiment and visually identified the pairs which we deemed as similar per experiment. Then we used Algorithms \ref{alg:asmmd} (ASMMD Algorithm) \& \ref{alg:similar_pairs} to confirm our inferences and if our initial groupings were off, we adjusted the similar and other nodes sets. As discussed in the previous section (\ref{chap:training_data}), the nodes were created out of two clusters, in each of which the nodes are similar to each other. Therefore, the similar nodes are either the nodes of the newly merged cluster or cluster 3. Similarly, we tested both possible similar nodes sets for each data configuration (balanced and unbalanced) to determine which one was best. 

Once we had an initial idea of the similar and other nodes sets, we could then use them to determine the kernel and bandwidth. As discussed above, we've used the ASMMD Algorithm to refine our similar and other nodes sets. However, we cannot determine these sets in isolation and we need to consider them in conjunction with the bandwidth and kernel and what similar pairs they produce. The two kernels we considered were the radial basis function (rbf) and linear kernel. We aimed to choose the parameters which would most effectively separate the similar from dissimilar pairs. The full parameter configuration of each dataset (, experiment) and data configuration is found in Table \ref{tab:asmmd_parameters}.



\subsection{Classifiers of Choice}

\begin{itemize}
    \item what classifiers we use per dataset
    \item what parameters we optimise and why
\end{itemize}

Using the humidity and temperature attributes of the dataset we have trained regression models to capture the relationship between humidity and temperature. The classifier of choice is Support Vector Regression Machine's (SVRs). SVRs are a version of SVM for regression proposed by Vapnik et al. \cite{OriginalSVR}. The adaptation is accomplished by introducing an $\epsilon$-insensitive region around the function, called the $\epsilon$-tube as shown in Figure \ref{fig-SVRex}. Similarly to other SVM methods, the hyperplane is represented in terms of support vectors, which are training samples that lie outside the boundary of the tube.  The optimization problem objective is to find the tube that best approximates the continuous-valued function, while balancing model complexity and prediction error. Therefore, the goal is to first minimise an $\epsilon$-insensitive loss function and find the flattest tube that contains most of the training instances. SVRs have a few variables of interest we want to optimise for each node model. It is worth nothing that we experiment with both the linear and rbf kernels in order to evaluate how different kernels interact with our framework. We optimise the regularization parameter and the epsilon in the epsilon-SVR model using grid search given a node's dataset.

\subsection{Model Reusability Metrics}

\begin{itemize}
    \item how exactly do we measure the performance time and why
    \item define discrepancy and explain how it relates to precision
\end{itemize}

Investigating the effectiveness of the framework, requires that we examine two aspects, the speedup we benefit from when we avoid training models for some nodes and the network, and the precision of the framework in terms of the recommendations it makes. We've defined both speedup and precision in the context of model reusability.

Starting off with the latter, precision needs to be assessed across three different levels. The precision of MMD at identifying good pairs for reusability, the precision of OCSVM at identifying the correct node to reuse it's model and lastly the combined precision of the framework. In order for MMD precision to be a meaningful measure to use, it is expressed in terms of how close the performance of using a proxy model is to the true model. We then consider this ratio with regards with a threshold and if it is above that threshold it is correct. The thresholds we considered were 0.85, 0.9 and 0.95. The OCSVM precision is either calculated strictly or with a 0.05 margin, that is if the node pointed by the direction of reusability does not yield the optimal performance, but it's performance is equal or less than 0.05 from the optimal, we consider that the framework has made the right decision. Then like the MMD precision, we consider the framework made the right decision if the ratio is above a threshold it is correct. For the combined precision we utilized lower values for the threshold, namely values 0.6 and 0.8. The reason that we assess precision in this way is to be able to gauge how effective each component of the framework is in isolation but also combined. Consequently, we can provide a more holistic evaluation of the framework. 

In terms of the speedup, we need to be able to quantify how much time did we save by not training some models with respect to what time we would need if we trained all of them. This requires that we first identify the nodes for which we won't train a model for. Our framework provides reusability solutions at the pair level but not at the network one. For this purpose we've defined a naive algorithm which given a set of pairs and the direction of reusability per pair it can provide us with the nodes which we do not need to train a model for, the model-less nodes. The algorithm is naive in the sense it simply aims to reduce the number of nodes for which we need to train models for. This does not necessarily translate into the "optimal solution" as it does not take into account any other variables of interest. The full solution is found under Algorithm \ref{alg:modelless_nodes}. It is worth noting that the speedup potential varies across datasets and samples. Hence, one sample may have the potential for a maximum of 40\% speedup, while another much more or less. Since we're using 100 samples per dataset it is impossible to know the exact speedup potential per sample hence we simply report it as a number. 

\begin{algorithm}
    \DontPrintSemicolon
    \caption{Finds nodes that can use a reused model based on the results of the framework
    }\label{alg:modelless_nodes}
    
    \KwData{$\boldsymbol{pair\_results}$: dictionary associating each pair with the node whose model to be reused i.e. the direction of reusability, $\boldsymbol{nodes}$: the list of nodes from the MMD identified pairs}
    \KwResult{$\boldsymbol{mns}$: modelless nodes i.e. nodes that do not require that a model is trained for them}
    
    \Begin{
    \SetKw{KwTo}{in}
         $similar\_pairs \longleftarrow pair\_results.keys()$ \;
         \tcp{model\_mns associates each modelless node (mn) with a list of potential replacement node models}
         $mns \longleftarrow nodes.copy()$, $model\_mns \longleftarrow \{\}$ \;
         \SetAlgoLined
         \For{node \KwTo nodes}{
            $model\_mns[node] \longleftarrow []$ \; 
         }
         \For{node \KwTo nodes}{
            $node\_similar\_pairs \longleftarrow get\_node\_similar\_pairs(node, similar\_pairs)$ \;
            \For{x, y \KwTo $node\_similar\_pairs$}{
                $model\_node \longleftarrow pair\_results[(x,y)]$ \;
                $mn \longleftarrow difference(model\_node, (x,y))$ \;
                \If{model   \_node \KwTo mns}{
                    $mns.remove(model\_node)$ \;
                }
                $model\_mns[mn].append(model\_node)$ \;
                \tcp{ensures we don't encounter the pair again}
                $similar\_pairs.pop((x,y))$\;
            }
         }
         \tcp{Replacement options for an mn that can be replaced themselves are not considered}
         \For{node \KwTo nodes}{
           \If{model\_mns[node].count() > 1}{
                \For{model\_node \KwTo model\_mns[node]}{
                    \If{model\_mns[node] not empty}{
                        $model\_mns[node].remove(model\_node)$ \;
                        $mns.append(model\_node)$
                    }
                }
           }
         }
            
    }
    
\end{algorithm}



\section{Results}

Explain how in addition to the model reusability metrics we'll use some figures based on the aggregated data from all samples.

\section{Discussion}



% Graphs are always good. I recommend getting to grips with Matlab, R or
% gnuplot rather than exporting horribly Excel bitmapped graphs.


% \begin{quotation}
%  The Assyrian came down like the wolf on the fold,
%  And his cohorts were gleaming in purple and gold;
%  And the sheen of their spears was like stars on the sea,
%  When the blue wave rolls nightly on deep Galilee.

%  Like the leaves of the forest when Summer is green,
%  That host with their banners at sunset were seen:
%  Like the leaves of the forest when Autumn hath blown,
%  That host on the morrow lay withered and strown.
% \end{quotation}

\section{Conclusions}

% The standard Lorem Ipsum passage, used since the 1500s

% ``Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.''
\vskip8pt \noindent
{\bf Acknowledgments.}
This is optional; it is a location for you to thank people

\bibliographystyle{acm}
\bibliography{thesis}

% \newpage
% \usepackage{appendix}
\clearpage
\appendix
\begin{table*}[]
\centering
\begin{tabular}{|cccccccccc|}
\hline
\multicolumn{10}{|c|}{\textbf{Experiment 1}}                                                                                                                                   \\ \hline
\multicolumn{1}{|c|}{\textbf{Node}} & \multicolumn{1}{c|}{\textbf{No. of Entries}} & \multicolumn{4}{c|}{\textbf{Humidity}}        & \multicolumn{4}{c|}{\textbf{Temperature}} \\ \hline
\multicolumn{1}{|c|}{\textbf{}}     & \multicolumn{1}{c|}{\textbf{}}               & Min & Max & Avg   & \multicolumn{1}{c|}{Std}  & Min     & Max     & Avg       & Std       \\
\multicolumn{1}{|c|}{pi2}           & \multicolumn{1}{c|}{1532}                    & 3   & 45  & 35.85 & \multicolumn{1}{c|}{6.57} & 15      & 57      & 27.6      & 11.08     \\
\multicolumn{1}{|c|}{pi3}           & \multicolumn{1}{c|}{899}                     & 19  & 33  & 28.53 & \multicolumn{1}{c|}{4.39} & 34      & 59      & 43.04     & 6.17      \\
\multicolumn{1}{|c|}{pi4}           & \multicolumn{1}{c|}{1766}                    & 18  & 45  & 35.47 & \multicolumn{1}{c|}{6.55} & 0       & 64      & 28.2      & 11.71     \\
\multicolumn{1}{|c|}{pi5}           & \multicolumn{1}{c|}{2078}                    & 18  & 37  & 27.51 & \multicolumn{1}{c|}{5.1}  & 20      & 63      & 39.78     & 6.9       \\ \hline
\multicolumn{10}{|c|}{\textbf{Experiment 2}}                                                                                                                                   \\ \hline
\multicolumn{1}{|c|}{pi2}           & \multicolumn{1}{c|}{580}                     & 26  & 56  & 48.48 & \multicolumn{1}{c|}{6.6}  & 21      & 49      & 28.42     & 3.86      \\
\multicolumn{1}{|c|}{pi3}           & \multicolumn{1}{c|}{807}                     & 27  & 49  & 41.73 & \multicolumn{1}{c|}{4.14} & 17      & 46      & 23.41     & 5.66      \\
\multicolumn{1}{|c|}{pi4}           & \multicolumn{1}{c|}{1021}                    & 30  & 59  & 50.56 & \multicolumn{1}{c|}{6.4}  & 16      & 36      & 22.76     & 2.76      \\
\multicolumn{1}{|c|}{pi5}           & \multicolumn{1}{c|}{1407}                    & 30  & 48  & 40.13 & \multicolumn{1}{c|}{2.73} & 20      & 49      & 28.05     & 3.36      \\ \hline
\multicolumn{10}{|c|}{\textbf{Experiment 3}}                                                                                                                                   \\ \hline
\multicolumn{1}{|c|}{pi2}           & \multicolumn{1}{c|}{342}                     & 33  & 42  & 38.39 & \multicolumn{1}{c|}{2.39} & 17      & 31      & 23.44     & 4.09      \\
\multicolumn{1}{|c|}{pi3}           & \multicolumn{1}{c|}{264}                     & 27  & 33  & 31.95 & \multicolumn{1}{c|}{0.77} & 34      & 43      & 38.63     & 1.26      \\
\multicolumn{1}{|c|}{pi4}           & \multicolumn{1}{c|}{488}                     & 20  & 39  & 31.24 & \multicolumn{1}{c|}{5.31} & 23      & 59      & 38.28     & 10.56     \\
\multicolumn{1}{|c|}{pi5}           & \multicolumn{1}{c|}{555}                     & 21  & 37  & 25.23 & \multicolumn{1}{c|}{3.91} & 20      & 47      & 38.13     & 4.65      \\ \hline
\end{tabular}
\caption{\label{tab:gnfuv}GNFUV Dataset Description}
\end{table*}

% Please add the following required packages to your document preamble:
\begin{table*}[]
\centering
\begin{tabular}{|c|c|cc|c|}
\hline
\textbf{\begin{tabular}[c]{@{}c@{}}Originating \\ Cluster\end{tabular}}             & \textbf{Node} & \textbf{yes} & \textbf{no} & \textbf{Total} \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Newly \\ Merged \\ Cluster\end{tabular}} & pi1           & 696          & 3510        & 4206           \\
                                                                                    & pi2           & 619          & 3587        & 4206           \\
                                                                                    & pi3           & 658          & 3548        & 4206           \\ \hline
\multirow{2}{*}{Cluster 3}                                                          & pi4           & 801          & 1224        & 2025           \\
                                                                                    & pi5           & 765          & 1258        & 2023           \\ \hline
\end{tabular}
\caption{\label{tab:BMnodes}BM Dataset Node Data Description}
\end{table*}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}





\end{document}